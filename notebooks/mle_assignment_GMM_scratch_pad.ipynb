{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8ddaf923-f7eb-408c-ae66-4c446411e034",
      "metadata": {
        "id": "8ddaf923-f7eb-408c-ae66-4c446411e034"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.mixture import BayesianGaussianMixture\n",
        "from dask import dataframe as dd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "195c4b4e-cf62-4ce9-9ed9-d1e34c85ceba",
      "metadata": {
        "id": "195c4b4e-cf62-4ce9-9ed9-d1e34c85ceba"
      },
      "outputs": [],
      "source": [
        "class DataTransformer():\n",
        "\n",
        "    \"\"\"\n",
        "    Transformer class responsible for processing data to train the CTABGANSynthesizer model\n",
        "\n",
        "    Variables:\n",
        "    1) train_data -> input dataframe\n",
        "    2) categorical_list -> list of categorical columns\n",
        "    3) mixed_dict -> dictionary of mixed columns\n",
        "    4) n_clusters -> number of modes to fit bayesian gaussian mixture (bgm) model\n",
        "    5) eps -> threshold for ignoring less prominent modes in the mixture model\n",
        "    6) ordering -> stores original ordering for modes of numeric columns\n",
        "    7) output_info -> stores dimension and output activations of columns (i.e., tanh for numeric, softmax for categorical)\n",
        "    8) output_dim -> stores the final column width of the transformed data\n",
        "    9) components -> stores the valid modes used by numeric columns\n",
        "    10) filter_arr -> stores valid indices of continuous component in mixed columns\n",
        "    11) meta -> stores column information corresponding to different data types i.e., categorical/mixed/numerical\n",
        "\n",
        "\n",
        "    Methods:\n",
        "    1) __init__() -> initializes transformer object and computes meta information of columns\n",
        "    2) get_metadata() -> builds an inventory of individual columns and stores their relevant properties\n",
        "    3) fit() -> fits the required bgm models to process the input data\n",
        "    4) transform() -> executes the transformation required to train the model\n",
        "    5) inverse_transform() -> executes the reverse transformation on data generated from the model\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train_data=pd.DataFrame, categorical_list=[], mixed_dict={}, n_clusters=10, eps=0.005):\n",
        "\n",
        "        self.meta = None\n",
        "        self.train_data = train_data\n",
        "        self.categorical_columns= categorical_list\n",
        "        self.mixed_columns= mixed_dict\n",
        "        self.n_clusters = n_clusters\n",
        "        self.eps = eps\n",
        "        self.ordering = []\n",
        "        self.output_info = []\n",
        "        self.output_dim = 0\n",
        "        self.components = []\n",
        "        self.filter_arr = []\n",
        "        self.meta = self.get_metadata()\n",
        "\n",
        "    def get_metadata(self):\n",
        "\n",
        "        meta = []\n",
        "\n",
        "        for index in range(self.train_data.shape[1]):\n",
        "            column = self.train_data.iloc[:,index]\n",
        "            if index in self.categorical_columns:\n",
        "                mapper = column.value_counts().index.tolist()\n",
        "                meta.append({\n",
        "                        \"name\": index,\n",
        "                        \"type\": \"categorical\",\n",
        "                        \"size\": len(mapper),\n",
        "                        \"i2s\": mapper\n",
        "                })\n",
        "            elif index in self.mixed_columns.keys():\n",
        "                meta.append({\n",
        "                    \"name\": index,\n",
        "                    \"type\": \"mixed\",\n",
        "                    \"min\": column.min(),\n",
        "                    \"max\": column.max(),\n",
        "                    \"modal\": self.mixed_columns[index]\n",
        "                })\n",
        "            else:\n",
        "                meta.append({\n",
        "                    \"name\": index,\n",
        "                    \"type\": \"continuous\",\n",
        "                    \"min\": column.min(),\n",
        "                    \"max\": column.max(),\n",
        "                })\n",
        "\n",
        "        return meta\n",
        "\n",
        "    def fit(self):\n",
        "\n",
        "        data = self.train_data.values\n",
        "\n",
        "        # stores the corresponding bgm models for processing numeric data\n",
        "        model = []\n",
        "\n",
        "        # iterating through column information\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            if info['type'] == \"continuous\":\n",
        "                # fitting bgm model\n",
        "                gm = BayesianGaussianMixture(\n",
        "                    n_components = self.n_clusters,\n",
        "                    weight_concentration_prior_type='dirichlet_process',\n",
        "                    weight_concentration_prior=0.001, # lower values result in lesser modes being active\n",
        "                    max_iter=100,n_init=1, random_state=42)\n",
        "                gm.fit(data[:, id_].reshape([-1, 1]))\n",
        "                model.append(gm)\n",
        "                # keeping only relevant modes that have higher weight than eps and are used to fit the data\n",
        "                old_comp = gm.weights_ > self.eps\n",
        "                mode_freq = (pd.Series(gm.predict(data[:, id_].reshape([-1, 1]))).value_counts().keys())\n",
        "                \n",
        "                comp = []\n",
        "                for i in range(self.n_clusters):\n",
        "                    if (i in (mode_freq)) & old_comp[i]:\n",
        "                        comp.append(True)\n",
        "                    else:\n",
        "                        comp.append(False)\n",
        "                self.components.append(comp)\n",
        "                self.output_info += [(1, 'tanh'), (np.sum(comp), 'softmax')]\n",
        "                self.output_dim += 1 + np.sum(comp)\n",
        "\n",
        "            elif info['type'] == \"mixed\":\n",
        "\n",
        "                # in case of mixed columns, two bgm models are used\n",
        "                gm1 = BayesianGaussianMixture(\n",
        "                    self.n_clusters,\n",
        "                    weight_concentration_prior_type='dirichlet_process',\n",
        "                    weight_concentration_prior=0.001, max_iter=100,\n",
        "                    n_init=1,random_state=42)\n",
        "                gm2 = BayesianGaussianMixture(\n",
        "                    self.n_clusters,\n",
        "                    weight_concentration_prior_type='dirichlet_process',\n",
        "                    weight_concentration_prior=0.001, max_iter=100,\n",
        "                    n_init=1,random_state=42)\n",
        "\n",
        "                # first bgm model is fit to the entire data only for the purposes of obtaining a normalized value of any particular categorical mode\n",
        "                gm1.fit(data[:, id_].reshape([-1, 1]))\n",
        "\n",
        "                # main bgm model used to fit the continuous component and serves the same purpose as with purely numeric columns\n",
        "                filter_arr = []\n",
        "                for element in data[:, id_]:\n",
        "                    if element not in info['modal']:\n",
        "                        filter_arr.append(True)\n",
        "                    else:\n",
        "                        filter_arr.append(False)\n",
        "                self.filter_arr.append(filter_arr)\n",
        "\n",
        "                gm2.fit(data[:, id_][filter_arr].reshape([-1, 1]))\n",
        "\n",
        "                model.append((gm1,gm2))\n",
        "\n",
        "                # similarly keeping only relevant modes with higher weight than eps and are used to fit strictly continuous data\n",
        "                old_comp = gm2.weights_ > self.eps\n",
        "                mode_freq = (pd.Series(gm2.predict(data[:, id_][filter_arr].reshape([-1, 1]))).value_counts().keys())\n",
        "                comp = []\n",
        "\n",
        "                for i in range(self.n_clusters):\n",
        "                    if (i in (mode_freq)) & old_comp[i]:\n",
        "                        comp.append(True)\n",
        "                    else:\n",
        "                        comp.append(False)\n",
        "\n",
        "                self.components.append(comp)\n",
        "\n",
        "                # modes of the categorical component are appended to modes produced by the main bgm model\n",
        "                self.output_info += [(1, 'tanh'), (np.sum(comp) + len(info['modal']), 'softmax')]\n",
        "                self.output_dim += 1 + np.sum(comp) + len(info['modal'])\n",
        "\n",
        "            else:\n",
        "                # in case of categorical columns, bgm model is ignored\n",
        "                model.append(None)\n",
        "                self.components.append(None)\n",
        "                self.output_info += [(info['size'], 'softmax')]\n",
        "                self.output_dim += info['size']\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def transform(self, data):\n",
        "\n",
        "        # stores the transformed values\n",
        "        values = []\n",
        "\n",
        "        # used for accessing filter_arr for transforming mixed columns\n",
        "        mixed_counter = 0\n",
        "\n",
        "        # iterating through column information\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            current = data[:, id_]\n",
        "            if info['type'] == \"continuous\":\n",
        "                # mode-specific normalization occurs here\n",
        "                current = current.reshape([-1, 1])\n",
        "                # means and stds of the modes are obtained from the corresponding fitted bgm model\n",
        "                means = self.model[id_].means_.reshape((1, self.n_clusters))\n",
        "                stds = np.sqrt(self.model[id_].covariances_).reshape((1, self.n_clusters))\n",
        "                # values are then normalized and stored for all modes\n",
        "                features = np.empty(shape=(len(current),self.n_clusters))\n",
        "                # note 4 is a multiplier to ensure values lie between -1 to 1 but this is not always guaranteed\n",
        "                features = (current - means) / (4 * stds)\n",
        "\n",
        "                # number of distict modes\n",
        "                n_opts = sum(self.components[id_])\n",
        "                # storing the mode for each data point by sampling from the probability mass distribution across all modes based on fitted bgm model\n",
        "                opt_sel = np.zeros(len(data), dtype='int')\n",
        "                probs = self.model[id_].predict_proba(current.reshape([-1, 1]))\n",
        "                probs = probs[:, self.components[id_]]\n",
        "                for i in range(len(data)):\n",
        "                    pp = probs[i] + 1e-6\n",
        "                    pp = pp / sum(pp)\n",
        "                    opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)\n",
        "\n",
        "                # creating a one-hot-encoding for the corresponding selected modes\n",
        "                probs_onehot = np.zeros_like(probs)\n",
        "                probs_onehot[np.arange(len(probs)), opt_sel] = 1\n",
        "\n",
        "                # obtaining the normalized values based on the appropriately selected mode and clipping to ensure values are within (-1,1)\n",
        "                idx = np.arange((len(features)))\n",
        "                features = features[:, self.components[id_]]\n",
        "                features = features[idx, opt_sel].reshape([-1, 1])\n",
        "                features = np.clip(features, -.99, .99)\n",
        "\n",
        "                # re-ordering the one-hot-encoding of modes in descending order as per their frequency of being selected\n",
        "                re_ordered_phot = np.zeros_like(probs_onehot)\n",
        "                col_sums = probs_onehot.sum(axis=0)\n",
        "                n = probs_onehot.shape[1]\n",
        "                largest_indices = np.argsort(-1*col_sums)[:n]\n",
        "                for id,val in enumerate(largest_indices):\n",
        "                    re_ordered_phot[:,id] = probs_onehot[:,val]\n",
        "\n",
        "                # storing the original ordering for invoking inverse transform\n",
        "                self.ordering.append(largest_indices)\n",
        "\n",
        "                # storing transformed numeric column represented as normalized values and corresponding modes\n",
        "                values += [features, re_ordered_phot]\n",
        "\n",
        "            elif info['type'] == \"mixed\":\n",
        "\n",
        "                # means and standard deviation of modes obtained from the first fitted bgm model\n",
        "                means_0 = self.model[id_][0].means_.reshape([-1])\n",
        "                stds_0 = np.sqrt(self.model[id_][0].covariances_).reshape([-1])\n",
        "\n",
        "                # list to store relevant bgm modes for categorical components\n",
        "                zero_std_list = []\n",
        "\n",
        "                # means and stds needed to normalize relevant categorical components\n",
        "                means_needed = []\n",
        "                stds_needed = []\n",
        "\n",
        "                # obtaining the closest bgm mode to the categorical component\n",
        "                for mode in info['modal']:\n",
        "                    # skipped for mode representing missing values\n",
        "                    if mode!=-9999999:\n",
        "                        dist = []\n",
        "                        for idx,val in enumerate(list(means_0.flatten())):\n",
        "                            dist.append(abs(mode-val))\n",
        "                        index_min = np.argmin(np.array(dist))\n",
        "                        zero_std_list.append(index_min)\n",
        "                    else: continue\n",
        "\n",
        "\n",
        "                # stores the appropriate normalized value of categorical modes\n",
        "                mode_vals = []\n",
        "\n",
        "                # based on the means and stds of the chosen modes for categorical components, their respective values are similarly normalized\n",
        "                for idx in zero_std_list:\n",
        "                    means_needed.append(means_0[idx])\n",
        "                    stds_needed.append(stds_0[idx])\n",
        "\n",
        "                for i,j,k in zip(info['modal'],means_needed,stds_needed):\n",
        "                    this_val  = np.clip(((i - j) / (4*k)), -.99, .99)\n",
        "                    mode_vals.append(this_val)\n",
        "\n",
        "                # for categorical modes representing missing values, the normalized value associated is simply 0\n",
        "                if -9999999 in info[\"modal\"]:\n",
        "                    mode_vals.append(0)\n",
        "\n",
        "                # transforming continuous component of mixed columns similar to purely numeric columns using second fitted bgm model\n",
        "                current = current.reshape([-1, 1])\n",
        "                filter_arr = self.filter_arr[mixed_counter]\n",
        "                current = current[filter_arr]\n",
        "\n",
        "                means = self.model[id_][1].means_.reshape((1, self.n_clusters))\n",
        "                stds = np.sqrt(self.model[id_][1].covariances_).reshape((1, self.n_clusters))\n",
        "\n",
        "                features = np.empty(shape=(len(current),self.n_clusters))\n",
        "                features = (current - means) / (4 * stds)\n",
        "\n",
        "                n_opts = sum(self.components[id_])\n",
        "                probs = self.model[id_][1].predict_proba(current.reshape([-1, 1]))\n",
        "                probs = probs[:, self.components[id_]]\n",
        "\n",
        "                opt_sel = np.zeros(len(current), dtype='int')\n",
        "                for i in range(len(current)):\n",
        "                    pp = probs[i] + 1e-6\n",
        "                    pp = pp / sum(pp)\n",
        "                    opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)\n",
        "\n",
        "                idx = np.arange((len(features)))\n",
        "                features = features[:, self.components[id_]]\n",
        "                features = features[idx, opt_sel].reshape([-1, 1])\n",
        "                features = np.clip(features, -.99, .99)\n",
        "\n",
        "                probs_onehot = np.zeros_like(probs)\n",
        "                probs_onehot[np.arange(len(probs)), opt_sel] = 1\n",
        "\n",
        "                # additional modes are appended to represent categorical component\n",
        "                extra_bits = np.zeros([len(current), len(info['modal'])])\n",
        "                temp_probs_onehot = np.concatenate([extra_bits,probs_onehot], axis = 1)\n",
        "\n",
        "                # storing the final normalized value and one-hot-encoding of selected modes\n",
        "                final = np.zeros([len(data), 1 + probs_onehot.shape[1] + len(info['modal'])])\n",
        "\n",
        "                # iterates through only the continuous component\n",
        "                features_curser = 0\n",
        "\n",
        "                for idx, val in enumerate(data[:, id_]):\n",
        "\n",
        "                    if val in info['modal']:\n",
        "                        # dealing with the modes of categorical component\n",
        "                        category_ = list(map(info['modal'].index, [val]))[0]\n",
        "                        final[idx, 0] = mode_vals[category_]\n",
        "                        final[idx, (category_+1)] = 1\n",
        "\n",
        "                    else:\n",
        "                        # dealing with the modes of continuous component\n",
        "                        final[idx, 0] = features[features_curser]\n",
        "                        final[idx, (1+len(info['modal'])):] = temp_probs_onehot[features_curser][len(info['modal']):]\n",
        "                        features_curser = features_curser + 1\n",
        "\n",
        "                # re-ordering the one-hot-encoding of modes in descending order as per their frequency of being selected\n",
        "                just_onehot = final[:,1:]\n",
        "                re_ordered_jhot= np.zeros_like(just_onehot)\n",
        "                n = just_onehot.shape[1]\n",
        "                col_sums = just_onehot.sum(axis=0)\n",
        "                largest_indices = np.argsort(-1*col_sums)[:n]\n",
        "\n",
        "                for id,val in enumerate(largest_indices):\n",
        "                      re_ordered_jhot[:,id] = just_onehot[:,val]\n",
        "\n",
        "                final_features = final[:,0].reshape([-1, 1])\n",
        "\n",
        "                # storing the original ordering for invoking inverse transform\n",
        "                self.ordering.append(largest_indices)\n",
        "\n",
        "                values += [final_features, re_ordered_jhot]\n",
        "\n",
        "                mixed_counter = mixed_counter + 1\n",
        "\n",
        "            else:\n",
        "                # for categorical columns, standard one-hot-encoding is applied where categories are in descending order of frequency by default\n",
        "                self.ordering.append(None)\n",
        "                col_t = np.zeros([len(data), info['size']])\n",
        "                idx = list(map(info['i2s'].index, current))\n",
        "                col_t[np.arange(len(data)), idx] = 1\n",
        "                values.append(col_t)\n",
        "\n",
        "        return np.concatenate(values, axis=1)\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "\n",
        "        # stores the final inverse transformed generated data\n",
        "        data_t = np.zeros([len(data), len(self.meta)])\n",
        "\n",
        "        # used to iterate through the columns of the raw generated data\n",
        "        st = 0\n",
        "\n",
        "        # iterating through original column information\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            if info['type'] == \"continuous\":\n",
        "\n",
        "                # obtaining the generated normalized values and clipping for stability\n",
        "                u = data[:, st]\n",
        "                u = np.clip(u, -1, 1)\n",
        "\n",
        "                # obtaining the one-hot-encoding of the modes representing the normalized values\n",
        "                v = data[:, st + 1:st + 1 + np.sum(self.components[id_])]\n",
        "\n",
        "                # re-ordering the modes as per their original ordering\n",
        "                order = self.ordering[id_]\n",
        "                v_re_ordered = np.zeros_like(v)\n",
        "                for id,val in enumerate(order):\n",
        "                    v_re_ordered[:,val] = v[:,id]\n",
        "                v = v_re_ordered\n",
        "\n",
        "                # ensuring un-used modes are represented with -100 such that they can be ignored when computing argmax\n",
        "                v_t = np.ones((data.shape[0], self.n_clusters)) * -100\n",
        "                v_t[:, self.components[id_]] = v\n",
        "                v = v_t\n",
        "\n",
        "                # obtaining approriate means and stds as per the appropriately selected mode for each data point based on fitted bgm model\n",
        "                means = self.model[id_].means_.reshape([-1])\n",
        "                stds = np.sqrt(self.model[id_].covariances_).reshape([-1])\n",
        "                p_argmax = np.argmax(v, axis=1)\n",
        "                std_t = stds[p_argmax]\n",
        "                mean_t = means[p_argmax]\n",
        "\n",
        "                # executing the inverse transformation\n",
        "                tmp = u * 4 * std_t + mean_t\n",
        "\n",
        "                data_t[:, id_] = tmp\n",
        "\n",
        "                # moving to the next set of columns in the raw generated data in correspondance to original column information\n",
        "                st += 1 + np.sum(self.components[id_])\n",
        "\n",
        "            elif info['type'] == \"mixed\":\n",
        "\n",
        "                # obtaining the generated normalized values and corresponding modes\n",
        "                u = data[:, st]\n",
        "                u = np.clip(u, -1, 1)\n",
        "                full_v = data[:,(st+1):(st+1)+len(info['modal'])+np.sum(self.components[id_])]\n",
        "\n",
        "                # re-ordering the modes as per their original ordering\n",
        "                order = self.ordering[id_]\n",
        "                full_v_re_ordered = np.zeros_like(full_v)\n",
        "                for id,val in enumerate(order):\n",
        "                    full_v_re_ordered[:,val] = full_v[:,id]\n",
        "                full_v = full_v_re_ordered\n",
        "\n",
        "                # modes of categorical component\n",
        "                mixed_v = full_v[:,:len(info['modal'])]\n",
        "\n",
        "                # modes of continuous component\n",
        "                v = full_v[:,-np.sum(self.components[id_]):]\n",
        "\n",
        "                # similarly ensuring un-used modes are represented with -100 to be ignored while computing argmax\n",
        "                v_t = np.ones((data.shape[0], self.n_clusters)) * -100\n",
        "                v_t[:, self.components[id_]] = v\n",
        "                v = np.concatenate([mixed_v,v_t], axis=1)\n",
        "                p_argmax = np.argmax(v, axis=1)\n",
        "\n",
        "                # obtaining the means and stds of the continuous component using second fitted bgm model\n",
        "                means = self.model[id_][1].means_.reshape([-1])\n",
        "                stds = np.sqrt(self.model[id_][1].covariances_).reshape([-1])\n",
        "\n",
        "                # used to store the inverse-transformed data points\n",
        "                result = np.zeros_like(u)\n",
        "\n",
        "                for idx in range(len(data)):\n",
        "                    # in case of categorical mode being selected, the mode value itself is simply assigned\n",
        "                    if p_argmax[idx] < len(info['modal']):\n",
        "                        argmax_value = p_argmax[idx]\n",
        "                        result[idx] = float(list(map(info['modal'].__getitem__, [argmax_value]))[0])\n",
        "                    else:\n",
        "                        # in case of continuous mode being selected, similar inverse-transform for purely numeric values is applied\n",
        "                        std_t = stds[(p_argmax[idx]-len(info['modal']))]\n",
        "                        mean_t = means[(p_argmax[idx]-len(info['modal']))]\n",
        "                        result[idx] = u[idx] * 4 * std_t + mean_t\n",
        "\n",
        "                data_t[:, id_] = result\n",
        "\n",
        "                st += 1 + np.sum(self.components[id_]) + len(info['modal'])\n",
        "\n",
        "            else:\n",
        "                # reversing one hot encoding back to label encoding for categorical columns\n",
        "                current = data[:, st:st + info['size']]\n",
        "                idx = np.argmax(current, axis=1)\n",
        "                data_t[:, id_] = list(map(info['i2s'].__getitem__, idx))\n",
        "                st += info['size']\n",
        "        return data_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "c443bba7",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataTransformer():\n",
        "\n",
        "    \"\"\"\n",
        "    Transformer class responsible for processing data to train the CTABGANSynthesizer model\n",
        "\n",
        "    Variables:\n",
        "    1) train_data -> input dataframe\n",
        "    2) categorical_list -> list of categorical columns\n",
        "    3) mixed_dict -> dictionary of mixed columns\n",
        "    4) n_clusters -> number of modes to fit bayesian gaussian mixture (bgm) model\n",
        "    5) eps -> threshold for ignoring less prominent modes in the mixture model\n",
        "    6) ordering -> stores original ordering for modes of numeric columns\n",
        "    7) output_info -> stores dimension and output activations of columns (i.e., tanh for numeric, softmax for categorical)\n",
        "    8) output_dim -> stores the final column width of the transformed data\n",
        "    9) components -> stores the valid modes used by numeric columns\n",
        "    10) filter_arr -> stores valid indices of continuous component in mixed columns\n",
        "    11) meta -> stores column information corresponding to different data types i.e., categorical/mixed/numerical\n",
        "\n",
        "\n",
        "    Methods:\n",
        "    1) __init__() -> initializes transformer object and computes meta information of columns\n",
        "    2) get_metadata() -> builds an inventory of individual columns and stores their relevant properties\n",
        "    3) fit() -> fits the required bgm models to process the input data\n",
        "    4) transform() -> executes the transformation required to train the model\n",
        "    5) inverse_transform() -> executes the reverse transformation on data generated from the model\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train_data=pd.DataFrame, categorical_list=[], mixed_dict={}, n_clusters=10, eps=0.005):\n",
        "\n",
        "        self.meta = None\n",
        "        self.train_data = train_data\n",
        "        self.categorical_columns= categorical_list\n",
        "        self.mixed_columns= mixed_dict\n",
        "        self.n_clusters = n_clusters\n",
        "        self.eps = eps\n",
        "        self.ordering = []\n",
        "        self.output_info = []\n",
        "        self.output_dim = 0\n",
        "        self.components = []\n",
        "        self.filter_arr = []\n",
        "        self.meta = self.get_metadata()\n",
        "\n",
        "    def get_metadata(self):\n",
        "\n",
        "        meta = []\n",
        "\n",
        "        for index in range(self.train_data.shape[1]):\n",
        "            column = self.train_data.iloc[:,index]\n",
        "            if index in self.categorical_columns:\n",
        "                mapper = column.value_counts().index.tolist()\n",
        "                meta.append({\n",
        "                        \"name\": index,\n",
        "                        \"type\": \"categorical\",\n",
        "                        \"size\": len(mapper),\n",
        "                        \"i2s\": mapper\n",
        "                })\n",
        "            elif index in self.mixed_columns.keys():\n",
        "                meta.append({\n",
        "                    \"name\": index,\n",
        "                    \"type\": \"mixed\",\n",
        "                    \"min\": column.min(),\n",
        "                    \"max\": column.max(),\n",
        "                    \"modal\": self.mixed_columns[index]\n",
        "                })\n",
        "            else:\n",
        "                meta.append({\n",
        "                    \"name\": self.train_data.columns[index],\n",
        "                    \"type\": \"continuous\",\n",
        "                    \"min\": float(column.min().compute()),\n",
        "                    \"max\": float(column.max().compute()),\n",
        "                })\n",
        "\n",
        "        return meta\n",
        "\n",
        "    def fit(self):\n",
        "\n",
        "        data = self.train_data\n",
        "\n",
        "        # stores the corresponding bgm models for processing numeric data\n",
        "        model = []\n",
        "\n",
        "        # iterating through column information\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            if info['type'] == \"continuous\":\n",
        "                # fitting bgm model\n",
        "                gm = BayesianGaussianMixture(\n",
        "                    n_components = self.n_clusters,\n",
        "                    weight_concentration_prior_type='dirichlet_process',\n",
        "                    weight_concentration_prior=0.001, # lower values result in lesser modes being active\n",
        "                    max_iter=100,n_init=1, random_state=42)\n",
        "                \n",
        "                gm.fit(data[[info['name']]])\n",
        "                model.append(gm)\n",
        "                # keeping only relevant modes that have higher weight than eps and are used to fit the data\n",
        "                old_comp = gm.weights_ > self.eps\n",
        "                mode_freq = (pd.Series(gm.predict(data[:, id_].reshape([-1, 1]))).value_counts().keys())\n",
        "                \n",
        "                comp = []\n",
        "                for i in range(self.n_clusters):\n",
        "                    if (i in (mode_freq)) & old_comp[i]:\n",
        "                        comp.append(True)\n",
        "                    else:\n",
        "                        comp.append(False)\n",
        "                self.components.append(comp)\n",
        "                self.output_info += [(1, 'tanh'), (np.sum(comp), 'softmax')]\n",
        "                self.output_dim += 1 + np.sum(comp)\n",
        "\n",
        "            elif info['type'] == \"mixed\":\n",
        "\n",
        "                # in case of mixed columns, two bgm models are used\n",
        "                gm1 = BayesianGaussianMixture(\n",
        "                    self.n_clusters,\n",
        "                    weight_concentration_prior_type='dirichlet_process',\n",
        "                    weight_concentration_prior=0.001, max_iter=100,\n",
        "                    n_init=1,random_state=42)\n",
        "                gm2 = BayesianGaussianMixture(\n",
        "                    self.n_clusters,\n",
        "                    weight_concentration_prior_type='dirichlet_process',\n",
        "                    weight_concentration_prior=0.001, max_iter=100,\n",
        "                    n_init=1,random_state=42)\n",
        "\n",
        "                # first bgm model is fit to the entire data only for the purposes of obtaining a normalized value of any particular categorical mode\n",
        "                gm1.fit(data[:, id_].reshape([-1, 1]))\n",
        "\n",
        "                # main bgm model used to fit the continuous component and serves the same purpose as with purely numeric columns\n",
        "                filter_arr = []\n",
        "                for element in data[:, id_]:\n",
        "                    if element not in info['modal']:\n",
        "                        filter_arr.append(True)\n",
        "                    else:\n",
        "                        filter_arr.append(False)\n",
        "                self.filter_arr.append(filter_arr)\n",
        "\n",
        "                gm2.fit(data[:, id_][filter_arr].reshape([-1, 1]))\n",
        "\n",
        "                model.append((gm1,gm2))\n",
        "\n",
        "                # similarly keeping only relevant modes with higher weight than eps and are used to fit strictly continuous data\n",
        "                old_comp = gm2.weights_ > self.eps\n",
        "                mode_freq = (pd.Series(gm2.predict(data[:, id_][filter_arr].reshape([-1, 1]))).value_counts().keys())\n",
        "                comp = []\n",
        "\n",
        "                for i in range(self.n_clusters):\n",
        "                    if (i in (mode_freq)) & old_comp[i]:\n",
        "                        comp.append(True)\n",
        "                    else:\n",
        "                        comp.append(False)\n",
        "\n",
        "                self.components.append(comp)\n",
        "\n",
        "                # modes of the categorical component are appended to modes produced by the main bgm model\n",
        "                self.output_info += [(1, 'tanh'), (np.sum(comp) + len(info['modal']), 'softmax')]\n",
        "                self.output_dim += 1 + np.sum(comp) + len(info['modal'])\n",
        "\n",
        "            else:\n",
        "                # in case of categorical columns, bgm model is ignored\n",
        "                model.append(None)\n",
        "                self.components.append(None)\n",
        "                self.output_info += [(info['size'], 'softmax')]\n",
        "                self.output_dim += info['size']\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def transform(self, data):\n",
        "\n",
        "        # stores the transformed values\n",
        "        values = []\n",
        "\n",
        "        # used for accessing filter_arr for transforming mixed columns\n",
        "        mixed_counter = 0\n",
        "\n",
        "        # iterating through column information\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            current = data[:, id_]\n",
        "            if info['type'] == \"continuous\":\n",
        "                # mode-specific normalization occurs here\n",
        "                current = current.reshape([-1, 1])\n",
        "                # means and stds of the modes are obtained from the corresponding fitted bgm model\n",
        "                means = self.model[id_].means_.reshape((1, self.n_clusters))\n",
        "                stds = np.sqrt(self.model[id_].covariances_).reshape((1, self.n_clusters))\n",
        "                # values are then normalized and stored for all modes\n",
        "                features = np.empty(shape=(len(current),self.n_clusters))\n",
        "                # note 4 is a multiplier to ensure values lie between -1 to 1 but this is not always guaranteed\n",
        "                features = (current - means) / (4 * stds)\n",
        "\n",
        "                # number of distict modes\n",
        "                n_opts = sum(self.components[id_])\n",
        "                # storing the mode for each data point by sampling from the probability mass distribution across all modes based on fitted bgm model\n",
        "                opt_sel = np.zeros(len(data), dtype='int')\n",
        "                probs = self.model[id_].predict_proba(current.reshape([-1, 1]))\n",
        "                probs = probs[:, self.components[id_]]\n",
        "                for i in range(len(data)):\n",
        "                    pp = probs[i] + 1e-6\n",
        "                    pp = pp / sum(pp)\n",
        "                    opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)\n",
        "\n",
        "                # creating a one-hot-encoding for the corresponding selected modes\n",
        "                probs_onehot = np.zeros_like(probs)\n",
        "                probs_onehot[np.arange(len(probs)), opt_sel] = 1\n",
        "\n",
        "                # obtaining the normalized values based on the appropriately selected mode and clipping to ensure values are within (-1,1)\n",
        "                idx = np.arange((len(features)))\n",
        "                features = features[:, self.components[id_]]\n",
        "                features = features[idx, opt_sel].reshape([-1, 1])\n",
        "                features = np.clip(features, -.99, .99)\n",
        "\n",
        "                # re-ordering the one-hot-encoding of modes in descending order as per their frequency of being selected\n",
        "                re_ordered_phot = np.zeros_like(probs_onehot)\n",
        "                col_sums = probs_onehot.sum(axis=0)\n",
        "                n = probs_onehot.shape[1]\n",
        "                largest_indices = np.argsort(-1*col_sums)[:n]\n",
        "                for id,val in enumerate(largest_indices):\n",
        "                    re_ordered_phot[:,id] = probs_onehot[:,val]\n",
        "\n",
        "                # storing the original ordering for invoking inverse transform\n",
        "                self.ordering.append(largest_indices)\n",
        "\n",
        "                # storing transformed numeric column represented as normalized values and corresponding modes\n",
        "                values += [features, re_ordered_phot]\n",
        "\n",
        "            elif info['type'] == \"mixed\":\n",
        "\n",
        "                # means and standard deviation of modes obtained from the first fitted bgm model\n",
        "                means_0 = self.model[id_][0].means_.reshape([-1])\n",
        "                stds_0 = np.sqrt(self.model[id_][0].covariances_).reshape([-1])\n",
        "\n",
        "                # list to store relevant bgm modes for categorical components\n",
        "                zero_std_list = []\n",
        "\n",
        "                # means and stds needed to normalize relevant categorical components\n",
        "                means_needed = []\n",
        "                stds_needed = []\n",
        "\n",
        "                # obtaining the closest bgm mode to the categorical component\n",
        "                for mode in info['modal']:\n",
        "                    # skipped for mode representing missing values\n",
        "                    if mode!=-9999999:\n",
        "                        dist = []\n",
        "                        for idx,val in enumerate(list(means_0.flatten())):\n",
        "                            dist.append(abs(mode-val))\n",
        "                        index_min = np.argmin(np.array(dist))\n",
        "                        zero_std_list.append(index_min)\n",
        "                    else: continue\n",
        "\n",
        "\n",
        "                # stores the appropriate normalized value of categorical modes\n",
        "                mode_vals = []\n",
        "\n",
        "                # based on the means and stds of the chosen modes for categorical components, their respective values are similarly normalized\n",
        "                for idx in zero_std_list:\n",
        "                    means_needed.append(means_0[idx])\n",
        "                    stds_needed.append(stds_0[idx])\n",
        "\n",
        "                for i,j,k in zip(info['modal'],means_needed,stds_needed):\n",
        "                    this_val  = np.clip(((i - j) / (4*k)), -.99, .99)\n",
        "                    mode_vals.append(this_val)\n",
        "\n",
        "                # for categorical modes representing missing values, the normalized value associated is simply 0\n",
        "                if -9999999 in info[\"modal\"]:\n",
        "                    mode_vals.append(0)\n",
        "\n",
        "                # transforming continuous component of mixed columns similar to purely numeric columns using second fitted bgm model\n",
        "                current = current.reshape([-1, 1])\n",
        "                filter_arr = self.filter_arr[mixed_counter]\n",
        "                current = current[filter_arr]\n",
        "\n",
        "                means = self.model[id_][1].means_.reshape((1, self.n_clusters))\n",
        "                stds = np.sqrt(self.model[id_][1].covariances_).reshape((1, self.n_clusters))\n",
        "\n",
        "                features = np.empty(shape=(len(current),self.n_clusters))\n",
        "                features = (current - means) / (4 * stds)\n",
        "\n",
        "                n_opts = sum(self.components[id_])\n",
        "                probs = self.model[id_][1].predict_proba(current.reshape([-1, 1]))\n",
        "                probs = probs[:, self.components[id_]]\n",
        "\n",
        "                opt_sel = np.zeros(len(current), dtype='int')\n",
        "                for i in range(len(current)):\n",
        "                    pp = probs[i] + 1e-6\n",
        "                    pp = pp / sum(pp)\n",
        "                    opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)\n",
        "\n",
        "                idx = np.arange((len(features)))\n",
        "                features = features[:, self.components[id_]]\n",
        "                features = features[idx, opt_sel].reshape([-1, 1])\n",
        "                features = np.clip(features, -.99, .99)\n",
        "\n",
        "                probs_onehot = np.zeros_like(probs)\n",
        "                probs_onehot[np.arange(len(probs)), opt_sel] = 1\n",
        "\n",
        "                # additional modes are appended to represent categorical component\n",
        "                extra_bits = np.zeros([len(current), len(info['modal'])])\n",
        "                temp_probs_onehot = np.concatenate([extra_bits,probs_onehot], axis = 1)\n",
        "\n",
        "                # storing the final normalized value and one-hot-encoding of selected modes\n",
        "                final = np.zeros([len(data), 1 + probs_onehot.shape[1] + len(info['modal'])])\n",
        "\n",
        "                # iterates through only the continuous component\n",
        "                features_curser = 0\n",
        "\n",
        "                for idx, val in enumerate(data[:, id_]):\n",
        "\n",
        "                    if val in info['modal']:\n",
        "                        # dealing with the modes of categorical component\n",
        "                        category_ = list(map(info['modal'].index, [val]))[0]\n",
        "                        final[idx, 0] = mode_vals[category_]\n",
        "                        final[idx, (category_+1)] = 1\n",
        "\n",
        "                    else:\n",
        "                        # dealing with the modes of continuous component\n",
        "                        final[idx, 0] = features[features_curser]\n",
        "                        final[idx, (1+len(info['modal'])):] = temp_probs_onehot[features_curser][len(info['modal']):]\n",
        "                        features_curser = features_curser + 1\n",
        "\n",
        "                # re-ordering the one-hot-encoding of modes in descending order as per their frequency of being selected\n",
        "                just_onehot = final[:,1:]\n",
        "                re_ordered_jhot= np.zeros_like(just_onehot)\n",
        "                n = just_onehot.shape[1]\n",
        "                col_sums = just_onehot.sum(axis=0)\n",
        "                largest_indices = np.argsort(-1*col_sums)[:n]\n",
        "\n",
        "                for id,val in enumerate(largest_indices):\n",
        "                      re_ordered_jhot[:,id] = just_onehot[:,val]\n",
        "\n",
        "                final_features = final[:,0].reshape([-1, 1])\n",
        "\n",
        "                # storing the original ordering for invoking inverse transform\n",
        "                self.ordering.append(largest_indices)\n",
        "\n",
        "                values += [final_features, re_ordered_jhot]\n",
        "\n",
        "                mixed_counter = mixed_counter + 1\n",
        "\n",
        "            else:\n",
        "                # for categorical columns, standard one-hot-encoding is applied where categories are in descending order of frequency by default\n",
        "                self.ordering.append(None)\n",
        "                col_t = np.zeros([len(data), info['size']])\n",
        "                idx = list(map(info['i2s'].index, current))\n",
        "                col_t[np.arange(len(data)), idx] = 1\n",
        "                values.append(col_t)\n",
        "\n",
        "        return np.concatenate(values, axis=1)\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "\n",
        "        # stores the final inverse transformed generated data\n",
        "        data_t = np.zeros([len(data), len(self.meta)])\n",
        "\n",
        "        # used to iterate through the columns of the raw generated data\n",
        "        st = 0\n",
        "\n",
        "        # iterating through original column information\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            if info['type'] == \"continuous\":\n",
        "\n",
        "                # obtaining the generated normalized values and clipping for stability\n",
        "                u = data[:, st]\n",
        "                u = np.clip(u, -1, 1)\n",
        "\n",
        "                # obtaining the one-hot-encoding of the modes representing the normalized values\n",
        "                v = data[:, st + 1:st + 1 + np.sum(self.components[id_])]\n",
        "\n",
        "                # re-ordering the modes as per their original ordering\n",
        "                order = self.ordering[id_]\n",
        "                v_re_ordered = np.zeros_like(v)\n",
        "                for id,val in enumerate(order):\n",
        "                    v_re_ordered[:,val] = v[:,id]\n",
        "                v = v_re_ordered\n",
        "\n",
        "                # ensuring un-used modes are represented with -100 such that they can be ignored when computing argmax\n",
        "                v_t = np.ones((data.shape[0], self.n_clusters)) * -100\n",
        "                v_t[:, self.components[id_]] = v\n",
        "                v = v_t\n",
        "\n",
        "                # obtaining approriate means and stds as per the appropriately selected mode for each data point based on fitted bgm model\n",
        "                means = self.model[id_].means_.reshape([-1])\n",
        "                stds = np.sqrt(self.model[id_].covariances_).reshape([-1])\n",
        "                p_argmax = np.argmax(v, axis=1)\n",
        "                std_t = stds[p_argmax]\n",
        "                mean_t = means[p_argmax]\n",
        "\n",
        "                # executing the inverse transformation\n",
        "                tmp = u * 4 * std_t + mean_t\n",
        "\n",
        "                data_t[:, id_] = tmp\n",
        "\n",
        "                # moving to the next set of columns in the raw generated data in correspondance to original column information\n",
        "                st += 1 + np.sum(self.components[id_])\n",
        "\n",
        "            elif info['type'] == \"mixed\":\n",
        "\n",
        "                # obtaining the generated normalized values and corresponding modes\n",
        "                u = data[:, st]\n",
        "                u = np.clip(u, -1, 1)\n",
        "                full_v = data[:,(st+1):(st+1)+len(info['modal'])+np.sum(self.components[id_])]\n",
        "\n",
        "                # re-ordering the modes as per their original ordering\n",
        "                order = self.ordering[id_]\n",
        "                full_v_re_ordered = np.zeros_like(full_v)\n",
        "                for id,val in enumerate(order):\n",
        "                    full_v_re_ordered[:,val] = full_v[:,id]\n",
        "                full_v = full_v_re_ordered\n",
        "\n",
        "                # modes of categorical component\n",
        "                mixed_v = full_v[:,:len(info['modal'])]\n",
        "\n",
        "                # modes of continuous component\n",
        "                v = full_v[:,-np.sum(self.components[id_]):]\n",
        "\n",
        "                # similarly ensuring un-used modes are represented with -100 to be ignored while computing argmax\n",
        "                v_t = np.ones((data.shape[0], self.n_clusters)) * -100\n",
        "                v_t[:, self.components[id_]] = v\n",
        "                v = np.concatenate([mixed_v,v_t], axis=1)\n",
        "                p_argmax = np.argmax(v, axis=1)\n",
        "\n",
        "                # obtaining the means and stds of the continuous component using second fitted bgm model\n",
        "                means = self.model[id_][1].means_.reshape([-1])\n",
        "                stds = np.sqrt(self.model[id_][1].covariances_).reshape([-1])\n",
        "\n",
        "                # used to store the inverse-transformed data points\n",
        "                result = np.zeros_like(u)\n",
        "\n",
        "                for idx in range(len(data)):\n",
        "                    # in case of categorical mode being selected, the mode value itself is simply assigned\n",
        "                    if p_argmax[idx] < len(info['modal']):\n",
        "                        argmax_value = p_argmax[idx]\n",
        "                        result[idx] = float(list(map(info['modal'].__getitem__, [argmax_value]))[0])\n",
        "                    else:\n",
        "                        # in case of continuous mode being selected, similar inverse-transform for purely numeric values is applied\n",
        "                        std_t = stds[(p_argmax[idx]-len(info['modal']))]\n",
        "                        mean_t = means[(p_argmax[idx]-len(info['modal']))]\n",
        "                        result[idx] = u[idx] * 4 * std_t + mean_t\n",
        "\n",
        "                data_t[:, id_] = result\n",
        "\n",
        "                st += 1 + np.sum(self.components[id_]) + len(info['modal'])\n",
        "\n",
        "            else:\n",
        "                # reversing one hot encoding back to label encoding for categorical columns\n",
        "                current = data[:, st:st + info['size']]\n",
        "                idx = np.argmax(current, axis=1)\n",
        "                data_t[:, id_] = list(map(info['i2s'].__getitem__, idx))\n",
        "                st += info['size']\n",
        "        return data_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "781c64a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = dd.read_csv(\"../data/raw/Credit.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "6cf2dd10",
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer = DataTransformer(train_data=data[['Amount']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "36ad719a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dask DataFrame Structure:\n",
            "                Amount\n",
            "npartitions=1         \n",
            "               float64\n",
            "                   ...\n",
            "Dask Name: getitem, 3 expressions\n",
            "Expr=ReadCSV(7f3a648)[['Amount']][['Amount']]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "InvalidIndexError",
          "evalue": "(slice(None, None, None), 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: (slice(None, None, None), 0)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[35], line 98\u001b[0m, in \u001b[0;36mDataTransformer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# keeping only relevant modes that have higher weight than eps and are used to fit the data\u001b[39;00m\n\u001b[0;32m     97\u001b[0m old_comp \u001b[38;5;241m=\u001b[39m gm\u001b[38;5;241m.\u001b[39mweights_ \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps\n\u001b[1;32m---> 98\u001b[0m mode_freq \u001b[38;5;241m=\u001b[39m (pd\u001b[38;5;241m.\u001b[39mSeries(gm\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreshape([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m])))\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    100\u001b[0m comp \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters):\n",
            "File \u001b[1;32mc:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\dask_expr\\_collection.py:413\u001b[0m, in \u001b[0;36mFrameBase.__getitem__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    412\u001b[0m     other \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\dask_expr\\_collection.py:4803\u001b[0m, in \u001b[0;36mnew_collection\u001b[1;34m(expr)\u001b[0m\n\u001b[0;32m   4801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_collection\u001b[39m(expr):\n\u001b[0;32m   4802\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create new collection from an expr\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4803\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta\u001b[49m\n\u001b[0;32m   4804\u001b[0m     expr\u001b[38;5;241m.\u001b[39m_name  \u001b[38;5;66;03m# Ensure backend is imported\u001b[39;00m\n\u001b[0;32m   4805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_collection_type(meta)(expr)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\functools.py:993\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    991\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 993\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    995\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
            "File \u001b[1;32mc:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\dask_expr\\_expr.py:2060\u001b[0m, in \u001b[0;36mProjection._meta\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2057\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[0;32m   2058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_meta\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   2059\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_dataframe_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39m_meta):\n\u001b[1;32m-> 2060\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta\u001b[49m\n\u001b[0;32m   2061\u001b[0m     \u001b[38;5;66;03m# if we are not a DataFrame and have a scalar, we reduce to a scalar\u001b[39;00m\n\u001b[0;32m   2062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperand(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mslice\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[0;32m   2063\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperand(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2064\u001b[0m     ):\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\functools.py:993\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    991\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 993\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    995\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
            "File \u001b[1;32mc:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\dask_expr\\_expr.py:495\u001b[0m, in \u001b[0;36mBlockwise._meta\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_meta\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    494\u001b[0m     args \u001b[38;5;241m=\u001b[39m [op\u001b[38;5;241m.\u001b[39m_meta \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, Expr) \u001b[38;5;28;01melse\u001b[39;00m op \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args]\n\u001b[1;32m--> 495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[1;32mc:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3811\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[1;32m-> 3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m   3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
            "\u001b[1;31mInvalidIndexError\u001b[0m: (slice(None, None, None), 0)"
          ]
        }
      ],
      "source": [
        "transformer.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8a73d674",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4    21833\n",
            "0    11766\n",
            "7     8420\n",
            "8     4692\n",
            "2     2031\n",
            "6      713\n",
            "1      305\n",
            "5       73\n",
            "3        9\n",
            "Name: count, dtype: int64\n",
            "Index([4, 0, 7, 8, 2, 6, 1, 5, 3], dtype='int64')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "transformer = DataTransformer(train_data=data[['Amount']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b643c80-9e5a-48bd-be20-f266f4ed8a08",
      "metadata": {
        "id": "7b643c80-9e5a-48bd-be20-f266f4ed8a08"
      },
      "source": [
        "# Improving GMM encoding for continuous value column"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb67b62-2b49-4cc5-a2eb-a1d3a086dbf3",
      "metadata": {
        "id": "9bb67b62-2b49-4cc5-a2eb-a1d3a086dbf3"
      },
      "source": [
        "Following we give an example of how to use our current gaussian mixture model to encode and decode continuous column. For this test, we have three demands:\n",
        "1. Current test is based on a small dataset. Please scale the code to enable GMM encoding on 1B rows data. This part is actually two sub-tasks (i) read 1B rows data into the algorithm and (ii) scale current GMM method to encode 1B rows data within a reasonable time.\n",
        "2. Properly evaluate the new GMM encoder. Make sure all the values can be inverse transformed. Especially the extreme values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "524d927f-89ea-42e6-82b8-6c79bc81188c",
      "metadata": {
        "id": "524d927f-89ea-42e6-82b8-6c79bc81188c"
      },
      "source": [
        "## Load data and encode column using gaussian mixture method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5fc166af-ea89-4844-80a3-fb6e56d718d5",
      "metadata": {
        "id": "5fc166af-ea89-4844-80a3-fb6e56d718d5"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"../data/raw/Credit.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b54e574e",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "137a9b73",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "404af909",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dask.distributed import Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3d1cf019",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
              "    <div style=\"margin-left: 48px;\">\n",
              "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
              "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-ca09920e-af09-11ef-b600-e865382f0554</p>\n",
              "        <table style=\"width: 100%; text-align: left;\">\n",
              "\n",
              "        <tr>\n",
              "        \n",
              "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
              "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> distributed.LocalCluster</td>\n",
              "        \n",
              "        </tr>\n",
              "\n",
              "        \n",
              "            <tr>\n",
              "                <td style=\"text-align: left;\">\n",
              "                    <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:8787/status\" target=\"_blank\">http://127.0.0.1:8787/status</a>\n",
              "                </td>\n",
              "                <td style=\"text-align: left;\"></td>\n",
              "            </tr>\n",
              "        \n",
              "\n",
              "        </table>\n",
              "\n",
              "        \n",
              "\n",
              "        \n",
              "            <details>\n",
              "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
              "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
              "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
              "    </div>\n",
              "    <div style=\"margin-left: 48px;\">\n",
              "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">LocalCluster</h3>\n",
              "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">292b6bb4</p>\n",
              "        <table style=\"width: 100%; text-align: left;\">\n",
              "            <tr>\n",
              "                <td style=\"text-align: left;\">\n",
              "                    <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:8787/status\" target=\"_blank\">http://127.0.0.1:8787/status</a>\n",
              "                </td>\n",
              "                <td style=\"text-align: left;\">\n",
              "                    <strong>Workers:</strong> 4\n",
              "                </td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                <td style=\"text-align: left;\">\n",
              "                    <strong>Total threads:</strong> 12\n",
              "                </td>\n",
              "                <td style=\"text-align: left;\">\n",
              "                    <strong>Total memory:</strong> 15.21 GiB\n",
              "                </td>\n",
              "            </tr>\n",
              "            \n",
              "            <tr>\n",
              "    <td style=\"text-align: left;\"><strong>Status:</strong> running</td>\n",
              "    <td style=\"text-align: left;\"><strong>Using processes:</strong> True</td>\n",
              "</tr>\n",
              "\n",
              "            \n",
              "        </table>\n",
              "\n",
              "        <details>\n",
              "            <summary style=\"margin-bottom: 20px;\">\n",
              "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
              "            </summary>\n",
              "\n",
              "            <div style=\"\">\n",
              "    <div>\n",
              "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
              "        <div style=\"margin-left: 48px;\">\n",
              "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
              "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-a467801f-7307-4844-a987-f691cb26eab6</p>\n",
              "            <table style=\"width: 100%; text-align: left;\">\n",
              "                <tr>\n",
              "                    <td style=\"text-align: left;\">\n",
              "                        <strong>Comm:</strong> tcp://127.0.0.1:52328\n",
              "                    </td>\n",
              "                    <td style=\"text-align: left;\">\n",
              "                        <strong>Workers:</strong> 4\n",
              "                    </td>\n",
              "                </tr>\n",
              "                <tr>\n",
              "                    <td style=\"text-align: left;\">\n",
              "                        <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:8787/status\" target=\"_blank\">http://127.0.0.1:8787/status</a>\n",
              "                    </td>\n",
              "                    <td style=\"text-align: left;\">\n",
              "                        <strong>Total threads:</strong> 12\n",
              "                    </td>\n",
              "                </tr>\n",
              "                <tr>\n",
              "                    <td style=\"text-align: left;\">\n",
              "                        <strong>Started:</strong> Just now\n",
              "                    </td>\n",
              "                    <td style=\"text-align: left;\">\n",
              "                        <strong>Total memory:</strong> 15.21 GiB\n",
              "                    </td>\n",
              "                </tr>\n",
              "            </table>\n",
              "        </div>\n",
              "    </div>\n",
              "\n",
              "    <details style=\"margin-left: 48px;\">\n",
              "        <summary style=\"margin-bottom: 20px;\">\n",
              "            <h3 style=\"display: inline;\">Workers</h3>\n",
              "        </summary>\n",
              "\n",
              "        \n",
              "        <div style=\"margin-bottom: 20px;\">\n",
              "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
              "            <div style=\"margin-left: 48px;\">\n",
              "            <details>\n",
              "                <summary>\n",
              "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 0</h4>\n",
              "                </summary>\n",
              "                <table style=\"width: 100%; text-align: left;\">\n",
              "                    <tr>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Comm: </strong> tcp://127.0.0.1:52357\n",
              "                        </td>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Total threads: </strong> 3\n",
              "                        </td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:52358/status\" target=\"_blank\">http://127.0.0.1:52358/status</a>\n",
              "                        </td>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Memory: </strong> 3.80 GiB\n",
              "                        </td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Nanny: </strong> tcp://127.0.0.1:52331\n",
              "                        </td>\n",
              "                        <td style=\"text-align: left;\"></td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
              "                            <strong>Local directory: </strong> C:\\Users\\suman\\AppData\\Local\\Temp\\dask-scratch-space\\worker-_8tjsc76\n",
              "                        </td>\n",
              "                    </tr>\n",
              "\n",
              "                    \n",
              "\n",
              "                    \n",
              "\n",
              "                </table>\n",
              "            </details>\n",
              "            </div>\n",
              "        </div>\n",
              "        \n",
              "        <div style=\"margin-bottom: 20px;\">\n",
              "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
              "            <div style=\"margin-left: 48px;\">\n",
              "            <details>\n",
              "                <summary>\n",
              "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 1</h4>\n",
              "                </summary>\n",
              "                <table style=\"width: 100%; text-align: left;\">\n",
              "                    <tr>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Comm: </strong> tcp://127.0.0.1:52351\n",
              "                        </td>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Total threads: </strong> 3\n",
              "                        </td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:52353/status\" target=\"_blank\">http://127.0.0.1:52353/status</a>\n",
              "                        </td>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Memory: </strong> 3.80 GiB\n",
              "                        </td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Nanny: </strong> tcp://127.0.0.1:52333\n",
              "                        </td>\n",
              "                        <td style=\"text-align: left;\"></td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
              "                            <strong>Local directory: </strong> C:\\Users\\suman\\AppData\\Local\\Temp\\dask-scratch-space\\worker-3bn8u_99\n",
              "                        </td>\n",
              "                    </tr>\n",
              "\n",
              "                    \n",
              "\n",
              "                    \n",
              "\n",
              "                </table>\n",
              "            </details>\n",
              "            </div>\n",
              "        </div>\n",
              "        \n",
              "        <div style=\"margin-bottom: 20px;\">\n",
              "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
              "            <div style=\"margin-left: 48px;\">\n",
              "            <details>\n",
              "                <summary>\n",
              "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 2</h4>\n",
              "                </summary>\n",
              "                <table style=\"width: 100%; text-align: left;\">\n",
              "                    <tr>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Comm: </strong> tcp://127.0.0.1:52360\n",
              "                        </td>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Total threads: </strong> 3\n",
              "                        </td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:52361/status\" target=\"_blank\">http://127.0.0.1:52361/status</a>\n",
              "                        </td>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Memory: </strong> 3.80 GiB\n",
              "                        </td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Nanny: </strong> tcp://127.0.0.1:52335\n",
              "                        </td>\n",
              "                        <td style=\"text-align: left;\"></td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
              "                            <strong>Local directory: </strong> C:\\Users\\suman\\AppData\\Local\\Temp\\dask-scratch-space\\worker-ni91eq3i\n",
              "                        </td>\n",
              "                    </tr>\n",
              "\n",
              "                    \n",
              "\n",
              "                    \n",
              "\n",
              "                </table>\n",
              "            </details>\n",
              "            </div>\n",
              "        </div>\n",
              "        \n",
              "        <div style=\"margin-bottom: 20px;\">\n",
              "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
              "            <div style=\"margin-left: 48px;\">\n",
              "            <details>\n",
              "                <summary>\n",
              "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 3</h4>\n",
              "                </summary>\n",
              "                <table style=\"width: 100%; text-align: left;\">\n",
              "                    <tr>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Comm: </strong> tcp://127.0.0.1:52352\n",
              "                        </td>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Total threads: </strong> 3\n",
              "                        </td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:52355/status\" target=\"_blank\">http://127.0.0.1:52355/status</a>\n",
              "                        </td>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Memory: </strong> 3.80 GiB\n",
              "                        </td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <td style=\"text-align: left;\">\n",
              "                            <strong>Nanny: </strong> tcp://127.0.0.1:52337\n",
              "                        </td>\n",
              "                        <td style=\"text-align: left;\"></td>\n",
              "                    </tr>\n",
              "                    <tr>\n",
              "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
              "                            <strong>Local directory: </strong> C:\\Users\\suman\\AppData\\Local\\Temp\\dask-scratch-space\\worker-837pf1op\n",
              "                        </td>\n",
              "                    </tr>\n",
              "\n",
              "                    \n",
              "\n",
              "                    \n",
              "\n",
              "                </table>\n",
              "            </details>\n",
              "            </div>\n",
              "        </div>\n",
              "        \n",
              "\n",
              "    </details>\n",
              "</div>\n",
              "\n",
              "        </details>\n",
              "    </div>\n",
              "</div>\n",
              "            </details>\n",
              "        \n",
              "\n",
              "    </div>\n",
              "</div>"
            ],
            "text/plain": [
              "<Client: 'tcp://127.0.0.1:52328' processes=4 threads=12, memory=15.21 GiB>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-01 06:46:14,040 - distributed.scheduler - WARNING - Worker failed to heartbeat for 33026s; attempting restart: <WorkerState 'tcp://127.0.0.1:52351', name: 1, status: running, memory: 0, processing: 0>\n",
            "2024-12-01 06:46:14,045 - distributed.scheduler - WARNING - Worker failed to heartbeat for 33026s; attempting restart: <WorkerState 'tcp://127.0.0.1:52352', name: 3, status: running, memory: 0, processing: 0>\n",
            "2024-12-01 06:46:14,045 - distributed.scheduler - WARNING - Worker failed to heartbeat for 33026s; attempting restart: <WorkerState 'tcp://127.0.0.1:52357', name: 0, status: running, memory: 0, processing: 0>\n",
            "2024-12-01 06:46:14,047 - distributed.scheduler - WARNING - Worker failed to heartbeat for 33026s; attempting restart: <WorkerState 'tcp://127.0.0.1:52360', name: 2, status: running, memory: 0, processing: 0>\n",
            "2024-12-01 06:46:15,973 - distributed.nanny - WARNING - Restarting worker\n",
            "2024-12-01 06:46:15,980 - distributed.nanny - WARNING - Restarting worker\n",
            "2024-12-01 06:46:16,240 - distributed.nanny - WARNING - Restarting worker\n",
            "2024-12-01 06:46:16,286 - distributed.nanny - WARNING - Restarting worker\n"
          ]
        }
      ],
      "source": [
        "client = Client()\n",
        "client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9de45e07",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = dd.read_csv(\"../data/raw/Credit.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "92e43218",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
              "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
              "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
              "       'Class'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4ded749f",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = data[['Time', 'Amount', 'Class']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3c85982d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "627.0"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "20064/32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0df94016",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "69.66666666666667"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "627/9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "732e6315",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']],\n",
              " Dask DataFrame Structure:\n",
              "                 Time   Amount  Class\n",
              " npartitions=1                       \n",
              "                int64  float64  int64\n",
              "                  ...      ...    ...\n",
              " Dask Name: getitem, 2 expressions\n",
              " Expr=ReadCSV(7f3a648)[['Time', 'Amount', 'Class']]]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[data]*70"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "afe55fae",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len([[[data]*70]*9]*32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "14ffdfba",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_concat = dd.concat([dd.concat([dd.concat([dd.concat([data]*70)])]*32)]*90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cc3f650f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50400"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "70*8*90"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ec7bdb90",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10048147200"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data_concat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "27281d9c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\distributed\\client.py:3371: UserWarning: Sending large graph of size 126.90 MiB.\n",
            "This may cause some slowdown.\n",
            "Consider loading the data with Dask directly\n",
            " or using futures or delayed objects to embed the data into the graph without repetition.\n",
            "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "data_concat.to_parquet('../data/scaled_only_amount/', compression='zstd',  write_index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "b2a2fb32",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_concat = dd.concat([[[data]*70]*9]*32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "3f85c7c8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20063.400345090486"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "((10**9)/49842)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f22e0c44-b7bb-4f42-b832-aa45fc5b248a",
      "metadata": {
        "id": "f22e0c44-b7bb-4f42-b832-aa45fc5b248a",
        "outputId": "ee319823-8f68-4484-9618-fe6388cc8044"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>197.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>23.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Amount\n",
              "0   14.61\n",
              "1    1.00\n",
              "2  197.04\n",
              "3    1.00\n",
              "4   23.25"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec4ad61",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e52c78f2-5e51-4548-9d1f-e1ae7660ef35",
      "metadata": {
        "id": "e52c78f2-5e51-4548-9d1f-e1ae7660ef35",
        "outputId": "7e1bb86b-41ae-432b-d485-6a1bc5065509"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n",
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n",
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n",
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n",
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n",
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n",
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n",
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n",
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n",
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n",
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n",
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n",
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n",
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n",
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n",
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n",
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n",
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n",
            "c:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m transformer \u001b[38;5;241m=\u001b[39m DataTransformer(train_data\u001b[38;5;241m=\u001b[39mtrain_data)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m transformed_train_data \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mtransform(train_data\u001b[38;5;241m.\u001b[39mvalues)\n",
            "Cell \u001b[1;32mIn[10], line 92\u001b[0m, in \u001b[0;36mDataTransformer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# fitting bgm model\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     gm \u001b[38;5;241m=\u001b[39m BayesianGaussianMixture(\n\u001b[0;32m     88\u001b[0m         n_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters,\n\u001b[0;32m     89\u001b[0m         weight_concentration_prior_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirichlet_process\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     90\u001b[0m         weight_concentration_prior\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;66;03m# lower values result in lesser modes being active\u001b[39;00m\n\u001b[0;32m     91\u001b[0m         max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 92\u001b[0m     \u001b[43mgm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     model\u001b[38;5;241m.\u001b[39mappend(gm)\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# keeping only relevant modes that have higher weight than eps and are used to fit the data\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:181\u001b[0m, in \u001b[0;36mBaseMixture.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mThe method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    The fitted mixture.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# parameters are validated in fit_predict\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:249\u001b[0m, in \u001b[0;36mBaseMixture.fit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    247\u001b[0m log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_e_step(X)\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_m_step(X, log_resp)\n\u001b[1;32m--> 249\u001b[0m lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_lower_bound\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_prob_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m change \u001b[38;5;241m=\u001b[39m lower_bound \u001b[38;5;241m-\u001b[39m prev_lower_bound\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_verbose_msg_iter_end(n_iter, change)\n",
            "File \u001b[1;32mc:\\Users\\suman\\data\\BD_MLE_assignment\\venv\\Lib\\site-packages\\sklearn\\mixture\\_bayesian_mixture.py:832\u001b[0m, in \u001b[0;36mBayesianGaussianMixture._compute_lower_bound\u001b[1;34m(self, log_resp, log_prob_norm)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     log_norm_weight \u001b[38;5;241m=\u001b[39m _log_dirichlet_norm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_concentration_)\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 832\u001b[0m     \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_resp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m log_resp)\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;241m-\u001b[39m log_wishart\n\u001b[0;32m    834\u001b[0m     \u001b[38;5;241m-\u001b[39m log_norm_weight\n\u001b[0;32m    835\u001b[0m     \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m n_features \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_precision_))\n\u001b[0;32m    836\u001b[0m )\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "transformer = DataTransformer(train_data=train_data)\n",
        "transformer.fit()\n",
        "transformed_train_data = transformer.transform(train_data.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "725a2f37-2f1b-45f8-abdb-6d212a9d662b",
      "metadata": {
        "id": "725a2f37-2f1b-45f8-abdb-6d212a9d662b"
      },
      "source": [
        "### Show the encoding for value 14.61"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8791b6a0-b607-4e86-b8e1-b87817116d40",
      "metadata": {
        "id": "8791b6a0-b607-4e86-b8e1-b87817116d40",
        "outputId": "22e37fe3-7596-449a-bc7d-b893d5334bb9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.44648112, 1.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        ])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformed_train_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "5240c344",
      "metadata": {},
      "outputs": [],
      "source": [
        "parquet = dd.read_parquet('../data/scaled/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "862f6f54",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1004814720"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(parquet)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac4362ca-7963-4e69-9477-16bf1f06085e",
      "metadata": {
        "id": "ac4362ca-7963-4e69-9477-16bf1f06085e"
      },
      "source": [
        "## Inverse transform back the encoded data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ddee28f2-ff90-460e-b835-b8cb76bc0511",
      "metadata": {
        "id": "ddee28f2-ff90-460e-b835-b8cb76bc0511"
      },
      "outputs": [],
      "source": [
        "inverse_transformed_train_data = transformer.inverse_transform(transformed_train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "15757369-273e-4c0a-aecd-96089bf0571f",
      "metadata": {
        "id": "15757369-273e-4c0a-aecd-96089bf0571f",
        "outputId": "c007fd30-75de-4a82-93f7-9913e253f1c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 14.61],\n",
              "       [  1.  ],\n",
              "       [197.04],\n",
              "       ...,\n",
              "       [ 12.  ],\n",
              "       [ 36.  ],\n",
              "       [108.  ]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inverse_transformed_train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1d92dd9-6a21-455b-a378-3962337de387",
      "metadata": {
        "id": "b1d92dd9-6a21-455b-a378-3962337de387",
        "outputId": "fe488773-7226-49a1-a63f-70cc025c6241"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>197.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>23.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Amount\n",
              "0   14.61\n",
              "1    1.00\n",
              "2  197.04\n",
              "3    1.00\n",
              "4   23.25"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(inverse_transformed_train_data, columns=[\"Amount\"]).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "008e9e14-b62f-44e8-9994-f205780015cb",
      "metadata": {
        "id": "008e9e14-b62f-44e8-9994-f205780015cb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
